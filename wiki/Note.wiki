#summary One-sentence summary of this page.

= MapTask =

*Add private method `runIterativeMapper`*

parse state data file and static data file

= IterativeMapper =

*Add new interface `IterativeMapper`*

= IncrIterHadoopTaskScheduler =

= job conf functions =

*`job.setMaintainState()`*

the reduce result (updated state) will be reused by the local map tasks, but we should keep the same number of map tasks and reduce tasks. So that there is a one-to-one mapping.

*`job.setReferenceData("path")`*

The reference data is the data that will be used during the computation process, the data will be loaded to each worker before the job starts.

*`job.setReferenceData("REDUCE_RESULT")`*

The reduce result is the updated reference data, which will be used in the next iteration. so after you set this, the system can load the data before iteration starts.

==Iterative Processing==

===Separating Static Data from Dynamic Data===

Here we have several kinds of key-value pairs:

* Static key and value <SK,SV>

represents a static data record

* Dynamic key and value <DK,DV>

represents a dynamic data record

* Intermediate key and value <IK,IV>

is the map output key-value pairs or the reduce input key-value pairs

* Output key and value <OK,OV>

is the reduce output key-value pairs

Then, our framework API supports the following key-value pairs transformations

* SK -> project -> DK

This specifies the relationship between a static data record and a dynamic data record. The system uses this projection to join the related static data record and dynamic data record. There are 3 relationships currently supported in our framework, ONE2ONE, ONE2ALL, ONE2MUL

* SK,SV,DV -> map -> IK,IV

The new map function will automatically join the static data record and dynamic data record, and provide them to user for them to implement the map function

* IK,[IV] -> reduce -> OK,OV

The traditional reduce function, which will reduce a list of values to a result value.

* [OK,OV] -> convert -> DK,DV

A convert function will transform the reduce output key-value pairs to the dynamic data record key-value pair.

Note that, in our system, we currently require IK=OK, which is very common case in iterative applications.


==Incremental Processing==

===Preserving Job===

Preserving job is the last job or the additional job when the normal iterative computation has finished. We use this job to record the converged state between mappers and reducers, which will be reused in the incremental jobs.

* change key-value pair processing system to key-value-source tuple processing system.

map output is <IK,IV>